input {
  file {
    path => "/usr/share/logstash/ingest_data/Air_Quality.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"      # 検証用。重複投入を防ぎたい運用では消す/固定パスにする
    mode => "read"                    # Logstash 8.7+ なら推奨（無ければ無視されるだけ）
  }
}

filter {
  csv {
    separator   => ","
    skip_header => true
    quote_char  => "\""
    columns => [
      "Unique ID","Indicator ID","Name","Measure","Measure Info",
      "Geo Type Name","Geo Join ID","Geo Place Name",
      "Time Period","Start_Date","Data Value","Message"
    ]
  }

  # 扱いやすいフィールド名に整形
  mutate {
    rename => {
      "Unique ID"      => "unique_id"
      "Indicator ID"   => "indicator_id"
      "Geo Type Name"  => "geo_type_name"
      "Geo Join ID"    => "geo_join_id"
      "Geo Place Name" => "geo_place_name"
      "Start_Date"     => "start_date"
      "Data Value"     => "data_value"
    }
    convert => {
      "unique_id"    => "integer"
      "indicator_id" => "integer"
      "geo_join_id"  => "integer"
      "data_value"   => "float"
    }
    strip => ["Name","Measure","Measure Info","geo_type_name","geo_place_name","Time Period","Message","start_date"]
  }

  # start_dateを@timestampに採用（Kibana の時系列で使う）
  date {
    match  => ["start_date","MM/dd/yyyy"]
    target => "@timestamp"
    timezone => "UTC"
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "air_quality"            # 単一インデックスに格納
    ilm_enabled => false              # 検証用。ILMを使うならtrueにしポリシーを定義する
  }
  # デバッグ用
  stdout { codec => rubydebug }
}
